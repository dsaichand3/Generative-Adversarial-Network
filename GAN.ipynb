{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vqGLmIcs9gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YhiGHJUtDd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "import tensorflow.keras.optimizers as Optimizer\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, LeakyReLU, BatchNormalization, Reshape, Conv2DTranspose, Conv2D, Input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01zv6uc5tEKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_real_images(directory):\n",
        "  images = []\n",
        "  for image in os.listdir(directory):\n",
        "    image = load_img(os.path.join(directory,image), target_size=(28,28))\n",
        "    image = img_to_array(image)\n",
        "    images.append(image)\n",
        "  return (np.array(images)/127.5)-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWmO7RmLttbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(7*7*256, use_bias=False, input_shape = (100,)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  \n",
        "  model.add(Reshape((7,7,256)))\n",
        "  assert model.output_shape == (None, 7, 7, 256)\n",
        "  \n",
        "  model.add(Conv2DTranspose(128, (5,5), strides = (1, 1), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  assert model.output_shape == (None, 7, 7, 128)\n",
        "  \n",
        "  model.add(Conv2DTranspose(64, (5,5), strides = (2, 2), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  assert model.output_shape == (None, 14, 14, 64)\n",
        "  \n",
        "  model.add(Conv2DTranspose(1, (5,5), strides = (2, 2), padding='same', activation='tanh'))\n",
        "  assert model.output_shape == (None, 28, 28, 1)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc74nqaptthw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32, (5,5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(64, (5,5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(128, (5,5), strides=(2, 2), padding='same'))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(256, (5,5), strides=(2, 2), padding='same'))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTTKzXYjttj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GAN(generator, discriminator):\n",
        "  noise = Input(shape=(100,))\n",
        "  generator_output = generator(noise)\n",
        "  discriminator_output = discriminator(generator_output)\n",
        "  gan = Model(noise, discriminator_output)\n",
        "  return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkz9ZFesxDTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator()\n",
        "\n",
        "discriminator = Discriminator()\n",
        "descriminator.compile(optimizer=Optimizer.Adam(lr=0.0003), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "gan = GAN(generator, discriminator)\n",
        "gan.compile(optimizer=Optimizer.Adam(lr=0.0003), loss='binary_crossentropy')\n",
        "\n",
        "real = np.ones((32,1))\n",
        "fake = np.zeros((32,1))\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "images = get_real_images('/content/100k/')\n",
        "\n",
        "for e in range(epochs):\n",
        "  \n",
        "  for i in range(len(images)//batch_size):\n",
        "    \n",
        "    Real_batch = images[i*batch_size:(i+1)*batch_size]\n",
        "    \n",
        "    noise = np.random.normal(loc=0, scale=1, size=(32, 100))\n",
        "    \n",
        "    real_loss = discriminator.train_on_batch(x = Real_batch, y = real)\n",
        "\n",
        "    Fake_batch = generator.predict_on_batch(noise)\n",
        "\n",
        "    fake_loss = discriminator.train_on_batch(x = Fake_batch, y = fake)\n",
        "\n",
        "    discriminator_loss = np.add(descri_loss_real, descri_loss_fake)/2\n",
        "\n",
        "    gan_loss = gan.train_on_batch(noise, real)\n",
        "\n",
        "    print(e, discriminator_loss, gan_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}